# üì¶ LLMxCPG Benchmarking System - Complete Setup

## ‚úÖ What Has Been Created

### 1. **Benchmark Dataset** (`benchmark/benchmark_dataset.json`)
   - ‚úÖ 10 comprehensive vulnerability test cases
   - ‚úÖ Covers diverse CWE categories (120, 416, 89, 190, 134, 22, 78, 415, 362, 476)
   - ‚úÖ Each test includes: code, ground truth queries, expected vulnerabilities
   - ‚úÖ Real-world vulnerability patterns

### 2. **Benchmark Execution Script** (`benchmark/run_benchmark.py`)
   - ‚úÖ Loads and tests multiple models in parallel
   - ‚úÖ Generates Joern CPG queries from vulnerable code
   - ‚úÖ Executes queries using Joern CPG tool
   - ‚úÖ Collects comprehensive metrics:
     - Query generation time
     - Query execution success rate
     - Vulnerability flow detection count
     - JSON validity checking
     - reachableByFlows usage detection
   - ‚úÖ Generates outputs:
     - HTML interactive report
     - CSV summary files
     - Detailed JSON results per model

### 3. **Training Automation Script** (`training/train_all_models.sh`)
   - ‚úÖ Trains all 3 models in sequence:
     - Qwen 2.5 7B Instruct
     - Qwen 2.5 Coder 14B
     - Qwen 2.5 Coder 32B
   - ‚úÖ Optimized batch sizes for A100 80GB:
     - 7B: batch_size=8, grad_accum=2 (effective 16)
     - 14B: batch_size=4, grad_accum=4 (effective 16)
     - 32B: batch_size=2, grad_accum=8 (effective 16)
   - ‚úÖ Automatic HuggingFace Hub push (optional)
   - ‚úÖ Color-coded progress output
   - ‚úÖ Comprehensive error handling

### 4. **HuggingFace Integration** (Updated `training/llmxcpg_query_finetune.py`)
   - ‚úÖ Added CLI arguments:
     - `--push_to_hub`: Enable pushing to Hub
     - `--hf_repo_id`: Target repository ID
     - `--hf_token`: Authentication token
   - ‚úÖ Dual repository strategy:
     - Pushes LoRA adapters to adapter repo
     - Pushes merged model to main repo
   - ‚úÖ Automatic authentication and error handling
   - ‚úÖ Progress logging

### 5. **Documentation**
   - ‚úÖ [benchmark/README.md](benchmark/README.md): Complete benchmarking guide
   - ‚úÖ [QUICKSTART.md](QUICKSTART.md): Step-by-step workflow
   - ‚úÖ This file: Setup summary

### 6. **Setup Script** (`setup.sh`)
   - ‚úÖ Interactive configuration wizard
   - ‚úÖ Sets HuggingFace username and token
   - ‚úÖ Updates training script automatically
   - ‚úÖ Checks dependencies
   - ‚úÖ Creates necessary directories
   - ‚úÖ Validates Joern installation

## üéØ Complete Workflow

### Option 1: Automated Setup (Recommended)
```bash
# Run setup wizard
./setup.sh

# This will:
# - Ask for HuggingFace username
# - Ask for HuggingFace token (optional)
# - Update all scripts with your username
# - Save token for automatic pushing
# - Check dependencies
# - Create directories
```

### Option 2: Manual Setup
```bash
# 1. Update HuggingFace username in training script
vim training/train_all_models.sh
# Replace "your-username" with your actual username

# 2. Set HuggingFace token
export HUGGING_FACE_TOKEN=hf_xxxxxxxxxx

# 3. Create directories
mkdir -p models benchmark/benchmark_results inference/results
```

## üöÄ Running the Complete Pipeline

### Step 1: Configure (One-Time)
```bash
./setup.sh
```

### Step 2: Train All Models
```bash
cd training
./train_all_models.sh
```

**This will:**
- Train Qwen 2.5 7B Instruct (~2-4 hours)
- Train Qwen 2.5 Coder 14B (~4-6 hours)
- Train Qwen 2.5 Coder 32B (~8-12 hours)
- Push all models to HuggingFace Hub (if token set)
- Save locally to `../models/`

### Step 3: Run Comprehensive Benchmark
```bash
cd ../benchmark
python run_benchmark.py \
    --models ../models/qwen2.5-7b-instruct-llmxcpg-query \
             ../models/qwen2.5-coder-14b-llmxcpg-query \
             ../models/qwen2.5-coder-32b-llmxcpg-query \
    --model_names "7B-Instruct" "14B-Coder" "32B-Coder" \
    --output_dir ./benchmark_results
```

**This will:**
- Test each model on 10 vulnerability types
- Generate Joern queries for each test
- Execute queries with Joern CPG
- Measure success rates and flow detection
- Generate comprehensive reports

### Step 4: View Results
```bash
# Open HTML report
open benchmark_results/benchmark_report.html

# View summaries
cat benchmark_results/benchmark_summary.csv
cat benchmark_results/per_test_comparison.csv
```

## üìä Expected Outputs

### During Training
```
ü§ñ Training Model 1/3: Qwen 2.5 7B Instruct
========================================
üì• Loading base model...
‚úÖ Model loaded successfully
üîß Applying LoRA adapters...
üéØ Training started...
  Step 100: loss=0.234
  Step 200: loss=0.198
‚úÖ Training complete!
üì§ Pushing to HuggingFace Hub...
‚úÖ Model pushed successfully!
```

### During Benchmarking
```
ü§ñ Benchmarking Model: 7B-Instruct
========================================
  üìù Test: buffer_overflow_01 - Buffer Overflow
     Generating queries... ‚úì 3 queries (2.1s)
     Creating CPG... ‚úì
     Executing queries with Joern...
       Query 1/3... ‚úì (0.8s, 5 flows)
       Query 2/3... ‚úì (1.2s, 8 flows)
       Query 3/3... ‚úì (0.9s, 2 flows)
     ‚úÖ Success rate: 100.0%, Total flows: 15
```

### Benchmark Report Summary
```
Model          | Avg Gen | Success | Flows | w/ Flows | Valid JSON
7B-Instruct    | 1.8s    | 87.3%   | 142   | 8/10     | 9/10
14B-Coder      | 3.2s    | 92.1%   | 178   | 9/10     | 10/10
32B-Coder      | 5.7s    | 95.6%   | 203   | 10/10    | 10/10
```

## üìÅ Final Directory Structure

```
llmxcpg/
‚îú‚îÄ‚îÄ benchmark/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_dataset.json          ‚úÖ 10 test cases
‚îÇ   ‚îú‚îÄ‚îÄ run_benchmark.py                ‚úÖ Benchmark script
‚îÇ   ‚îú‚îÄ‚îÄ README.md                       ‚úÖ Documentation
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_results/
‚îÇ       ‚îú‚îÄ‚îÄ benchmark_report.html       ‚Üê Generated
‚îÇ       ‚îú‚îÄ‚îÄ benchmark_summary.csv       ‚Üê Generated
‚îÇ       ‚îú‚îÄ‚îÄ per_test_comparison.csv     ‚Üê Generated
‚îÇ       ‚îú‚îÄ‚îÄ 7B-Instruct_results.json    ‚Üê Generated
‚îÇ       ‚îú‚îÄ‚îÄ 14B-Coder_results.json      ‚Üê Generated
‚îÇ       ‚îî‚îÄ‚îÄ 32B-Coder_results.json      ‚Üê Generated
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ llmxcpg_query_finetune.py       ‚úÖ Updated with HF push
‚îÇ   ‚îú‚îÄ‚îÄ train_all_models.sh             ‚úÖ Automated training
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ qwen2.5-7b-instruct-llmxcpg-query/    ‚Üê Generated
‚îÇ   ‚îú‚îÄ‚îÄ qwen2.5-coder-14b-llmxcpg-query/      ‚Üê Generated
‚îÇ   ‚îî‚îÄ‚îÄ qwen2.5-coder-32b-llmxcpg-query/      ‚Üê Generated
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ query_inference.py              ‚Üê Existing
‚îÇ   ‚îî‚îÄ‚îÄ quick_test.py                   ‚Üê Existing
‚îú‚îÄ‚îÄ setup.sh                             ‚úÖ Setup wizard
‚îú‚îÄ‚îÄ QUICKSTART.md                        ‚úÖ Quick guide
‚îî‚îÄ‚îÄ SETUP_SUMMARY.md                     ‚úÖ This file
```

## üîß System Requirements

### Hardware
- ‚úÖ NVIDIA A100 80GB GPU (as per your setup)
- ‚úÖ 200GB+ disk space for models
- ‚úÖ 32GB+ RAM

### Software
- ‚úÖ Python 3.8+
- ‚úÖ CUDA 11.8+
- ‚úÖ Joern CPG tool

### Python Packages
```bash
pip install torch transformers unsloth pandas huggingface_hub
```

## üéì Model Configurations

### Qwen 2.5 7B Instruct
- Base: `unsloth/Qwen2.5-7B-Instruct`
- Batch size: 8, Gradient accumulation: 2
- LoRA rank: 128, alpha: 256
- Training time: ~2-4 hours
- Best for: Production deployment (fast + accurate)

### Qwen 2.5 Coder 14B
- Base: `Qwen/Qwen2.5-Coder-14B-Instruct`
- Batch size: 4, Gradient accumulation: 4
- LoRA rank: 128, alpha: 256
- Training time: ~4-6 hours
- Best for: Balanced performance

### Qwen 2.5 Coder 32B
- Base: `Qwen/Qwen2.5-Coder-32B-Instruct`
- Batch size: 2, Gradient accumulation: 8
- LoRA rank: 128, alpha: 256
- Training time: ~8-12 hours
- Best for: Research (highest accuracy)

## üìà Benchmark Metrics Explained

### Generation Metrics
- **Generation Time**: Time to generate queries (lower = faster)
- **Query Count**: Number of queries generated per test
- **Valid JSON**: Whether output is parseable JSON

### Execution Metrics
- **Success Rate**: % of queries that execute without errors
- **Flow Count**: Number of vulnerability flows detected
- **Has reachableByFlows**: Whether queries use data flow analysis

### Quality Indicators
- Success rate >80%: Good model
- Flow count: Higher = better vulnerability detection
- reachableByFlows usage: Critical for vulnerability detection

## üêõ Troubleshooting

### "Joern not found"
```bash
# Install Joern
wget https://github.com/joernio/joern/releases/latest/download/joern-install.sh
chmod +x joern-install.sh
./joern-install.sh

# Add to PATH
export PATH=$PATH:~/joern/joern-cli

# Or specify path explicitly
python run_benchmark.py --joern_path /path/to/joern ...
```

### "Out of memory during training"
```bash
# For 32B model, reduce batch size
# Edit train_all_models.sh line for 32B:
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 16
```

### "HuggingFace push failed"
```bash
# Login to HuggingFace
huggingface-cli login

# Or set token
export HUGGING_FACE_TOKEN=hf_xxxxx

# Verify token
huggingface-cli whoami
```

### "Training stops at 1 iteration"
This is expected behavior when `max_steps=0` (trains full dataset).
If training actually stops early, check:
```python
# In llmxcpg_query_finetune.py
# Make sure max_steps is 0 or removed
trainer = SFTTrainer(
    ...
    # max_steps=0,  # Should be 0 or commented out
    ...
)
```

## üìö Next Steps After Benchmarking

1. **Analyze Results**: Review HTML report and identify best model
2. **Deploy Best Model**: Use for production inference
3. **Fine-tune**: Adjust hyperparameters based on results
4. **Expand Tests**: Add more vulnerability types to benchmark
5. **Share Results**: Push models and report to HuggingFace

## ü§ù Contributing

To add test cases:
1. Edit `benchmark/benchmark_dataset.json`
2. Add entry with: id, name, CWE, code, ground_truth_queries
3. Test manually with Joern first
4. Run benchmark and verify results

## üìß Support

- Benchmark issues: Check [benchmark/README.md](benchmark/README.md)
- Training issues: Check [training/README.md](training/README.md)
- Quick reference: See [QUICKSTART.md](QUICKSTART.md)

## ‚úÖ Verification Checklist

Before starting, verify:
- [ ] Setup script run: `./setup.sh`
- [ ] HuggingFace token set
- [ ] Training dataset exists: `data/llmxcpg_query_train.json`
- [ ] Joern installed and in PATH
- [ ] A100 GPU available
- [ ] 200GB+ disk space
- [ ] Python dependencies installed

## üéâ Ready to Go!

Everything is set up and ready. Run:

```bash
# Quick start
./setup.sh              # Configure
cd training             # Navigate
./train_all_models.sh   # Train (12-24 hours total)
cd ../benchmark         # Navigate
python run_benchmark.py --models ../models/qwen* --model_names 7B 14B 32B
open benchmark_results/benchmark_report.html
```

**Total pipeline time:** ~15-30 hours (training + benchmarking)

Good luck with your LLMxCPG benchmarking! üöÄ
